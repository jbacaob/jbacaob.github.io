---
layout: post
title: Solving an optimal growth model using reinforcement learning
date: 2024-12-19
permalink: /posts/rl_ogm/
tags: [macroeconomics, reinforcement learning]
category: Computational macroeconomics
math: true
---

We employ Q-learning to solve a deterministic optimal growth model. 
We then compare our results to the value function implied by the value function iteration 
approach.

## Optimal growth model: The basics

Consider an economy populated by identical households. The social planner chooses capital and consumption
streams $$\{c_t,k_{t+1}\}_{t=0}^\infty$$ to maximize their lifetime utility:

$$
V(k_0) = \sum_{t=0}^\infty u(c_t)
$$

subject to the resource constraint $$c_t + k_{t+1} = f(k_t) + (1-\delta)k_t$$, given an initial value $k_0$. As usual, we assume
that the functions $u$ and $f$ satisfy the Inada conditions $u'>0, u^"<0, f(0)=0, f'>0, f^"\leq 0$.

The only state variable in period $t$ is $k$. The controls are $c,k'$. Given the resource constraint, we focus
our attention on $k'$, so long as we make sure that $c$ is nonnegative.

For this problem, the Bellman equation takes the form:

$$
\begin{equation} \label{bellman}
V(k) = \max_{0\leq k' \leq f(k) + (1-\delta)k}\left\{u\left(f(k) + (1-\delta)k - k'\right) + \beta V(k')\right\}
\end{equation}
$$

Reference equation \eqref{bellman}

In the language of Markov decision problems we notice that:
- The action space $\mathcal{A}$ and state space $\mathcal{X}$ are identical and assumed to be a finite set of positive reals $$\mathcal{A} = \mathcal{X} = \{k_1,k_2,\ldots,k_N\}$$.
- The feasible correspondence set is $$\Gamma(k) = \left\{k \in X:0 \leq a \leq f(k) +(1-\delta)k\right\}\ \forall\ k\in \mathcal{X}$$. 
- The feasible state action pairs is given by $$G = \{(k,a) \in \mathcal{A}\times \mathcal{X}: 0 \leq a \leq f(k) +(1-\delta)k \}$$.
- The transition kernel can be defined as $$P(k, a, k') = \mathbb{1}\{k'= a\}$$. 
- The rewards function is implied by the instantaneous utility of consumption $r(k,a) = u(f(k) + (1-\delta)k - a)$. The discount factor is $\beta \in (0,1)$.

This means that the Bellman operator

$$
\begin{aligned}
(Tv)(k) &\equiv \max_{a'\in \Gamma(k)}\left\{u\left(f(k) + (1-\delta)k - a\right) + \beta \sum_{k'}v(k')P(k,a,k')   \right\} \\
        &= \max_{0\leq k' \leq f(k) + (1-\delta)k}\left\{u\left(f(k) + (1-\delta)k - k'\right) + \beta v(k')\right\}
\end{aligned}
$$

inherits the nice property that $$T^kv\rightarrow v^*$$ as $$k\rightarrow \infty$$ for any $v \in \mathbb{R}^{\mathcal{X}}$. Also,
it turns out $v^*$ is the unique solution of the Bellman equation above.

More formally, it turns out that $T$ is a contraction mapping of modulus $\beta$ on the closed set $\mathbb{R}^{\mathcal{X}} \equiv \{\}$. According to Banach's contraction mapping theorem, then $T$ has a unique fixed point $$v^*$$ on $$\mathbb{R}^{\mathcal{X}}$$ which is also globally stable. The fact that $$v^*$$ is the unique solution of the Bellman equation follows straightforwardly. A thorough discussion of Markov decision problems, accompanied by a detailed proof of the aforementioned facts
can be found at [^1].

Our goal now is two introduce two techniques to find $$v^*$$. The first one, value function iteration, is widespread know and we don't delve too much on it. The second one, a form of reinforcement learning named _Q-learning_ is relatively unknown in the economics community so we devote more time to it.

## Dynamic programming

We describe below the value function iteration algorithm:

- Initialize $v_0$, an initial guess of the value function
- Input a tolerance level $\tau$, a large error $\varepsilon$, and a maximum number of iterations $i_{max}$
- Initialize first iteration $$i$$: $$i \leftarrow 0$$
- While $$i < i_{max}$$ and $$\varepsilon > \tau$$:
    - Update step $k$ estimate of the value function: $$v_{k+1} \leftarrow Tv_k$$
    - Update error: $$\varepsilon \leftarrow \lVert v_{k+1} - v_k \rVert_{\infty}$$
    - Move to next iteration: $$i \leftarrow i + 1$$ 
- Retrieve the value function $v^*$ desired.

The choice of tolerance means that we can get arbitrarily closer to the value function, i.e. the solution of the Bellman equation 


```python
import pandas as pd
```

## Q-learning

One form of reinforcement learning is Q-learning. An excellent introduction to reinforcement learning techniques, as well as a detailed explanation of Q-learning, is contained in [^2].

One form of reinforcement learning is Q-learning. To draw some important characteristics of Q-learning, let's write down the 
corresponding algorithm

Q-learning is an off-policy temporal difference control algorithm. 
This means that

- Input the learning rate $$\theta \in (0,1]$$, a small exploration probability $$\epsilon$$, and the initial $q$-table $$q_0(k,k')$$ for all $$k, k' \in \mathcal{X}$$
- In the $$i$$-th episode:
    1. Choose a state $$k$$ randomly
    2. For each epoch of the episode:
        1. Choose action $$k'$$ using an $$\epsilon$$-greedy algorithm
        2. Take action $$k'$$, observe reward $$u$$
        3. Compute temporal difference for the state-action pair $$(k,k')$$
        4. Update Q-table: $$q_{k+1}(k,k') \leftarrow q_k(k,k') + \theta\left[u + \beta \max_{a}q(k',a) - q_k(k,k')\right]$$
        5. Update state: $$k \leftarrow k'$$
        6. Update error until convergence or maximum number of epochs: $$\varepsilon \leftarrow \lVert q_{k+1} - q_k \rVert_{\infty}$$
    3. Return Q-table
- Move to $$(i+1)$$-th episode using Q-table of $$i$$-th episode
- Iterate until convergence or until maximum number of episodes is achieved.


Note that $Q$-learning uses a policy different from the target policy to generate behavior. The behavior policy consists of an
$$\epsilon$$-greedy strategy that gives room for the social planner to explore and learn about the environment (step 4.2.1). Nonetheless, the target policy used to update the $q$-table is purely greedy, i.e., selects the action $a$ that maximizes $$q(k',a)$$. For this reason $Q$-learning is said to be _off policy_.

```python

class QRBC:
    def __init__(self,
                 α=0.33, 
                 β=0.95, 
                 δ=0.10, 
                 σ=2.0, 
                 grid_size=10, 
                 lr=0.5, 
                 eps=0.1, 
                 δ_tol=1e-5, 
                 T=20000,
                 seed=None):
        
        self.α, self.β, self.δ, self.σ = α, β, δ, σ
        self.grid_size = grid_size
        self.lr = lr
        self.eps = eps
        self.δ_tol = δ_tol
        self.T = T

        self.kstar = (α / (1 / β - (1 - δ))) ** (1 / (1 - α))
        self.grid = np.linspace(0.25 * self.kstar, 1.75 * self.kstar, grid_size)
        self.qtable = np.zeros((grid_size, grid_size))

        if seed is not None:
            np.random.seed(seed)

    def production(self, k):
        "Production function f(k)"
        return k**self.α

    def u(self, c):
        "The utility function"
        if self.σ == 1:
            return np.log(c)
        else:
            return (c**(1 - self.σ) - 1) / (1 - self.σ)

    def select_state(self):
        return np.random.randint(0, len(self.grid))

    def temp_diff(self, state, action):
        '''
        Compute the temporal difference for a given state-action pair
        '''
        state_next = np.argmin(np.abs(self.grid - self.grid[action]))
        c = max(self.production(self.grid[state]) + (1 - self.δ) * self.grid[state] - self.grid[action], 1e-8)     #Consumption
        TD = self.u(c) + self.β * np.max(self.qtable[state_next, :]) - self.qtable[state, action]
        return TD, state_next

    def run_one_epoch(self, max_times=10000):
        '''
        Run a single epoch of Q-learning
        '''
        s = self.select_state()
        for t in range(max_times):
            # Choose action
            if np.random.rand() < self.eps:
                action = np.random.randint(0, self.grid_size)  # Explore
            else:
                action = np.argmax(self.qtable[s, :])  # Exploit

            TD, s_next = self.temp_diff(s, action)

            # Update Q-table
            qtable_new = self.qtable.copy()
            qtable_new[s, action] = self.qtable[s, action] + self.lr * TD

            # Check convergence
            diff = np.abs(qtable_new - self.qtable).max()
            if diff < self.δ_tol:
                break
            s, self.qtable = s_next, qtable_new

        return self.qtable
```

```python

def run_epochs(N, qrbc):
    '''
    Run N epochs of Q-learning with qtable of last iteration each time
    '''
    for i in range(N):
        if i % (N / 10) == 0:
            print(f"Progress: EPOCHS = {i}")
        new_qtable = qrbc.run_one_epoch()

    return new_qtable
```

```python
def valfunc_from_qtable(qtable):
    '''
    Compute the value function from the Q-table
    '''
    V = np.max(qtable, axis=1)
    return V
```

------
[^1]: Sargent, T., & Stachurski, J. (2024). [Dynamic Programming. Volume I: Finite States](https://dp.quantecon.org/).
[^2]: Sutton, R., & Barto, A. (2020). [Reinforcement Learning: An Introduction](https://incompleteideas.net/book/the-book-2nd.html). Second edition. 